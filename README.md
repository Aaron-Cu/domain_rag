# domain_rag

**domain_rag** is a utility pipeline for processing domain-specific documents (e.g., academic papers, reports) and converting them into structured and plain-text formats suitable for Retrieval-Augmented Generation (RAG) systems. It leverages GROBID for PDF to XML transformation and custom tools for extracting and cleaning the text for downstream tasks such as document retrieval or fine-tuning.

---

## ğŸ“† Project Structure

```
domain_rag/
|
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/               # Input folder for raw PDF documents
â”‚   â””â”€â”€ xml_output/         # Output folder for GROBID XML files
|
â”œâ”€â”€ helper/
â”‚   â”œâ”€â”€ grobid_utils.py     # Functions to run GROBID on PDFs
â”‚   â””â”€â”€ tei_utils.py        # Functions to extract and clean TEI XML
|
â”œâ”€â”€ README.md               # This file
â””â”€â”€ ...
```

---

## âš™ï¸ Utilities

### GROBID PDF Processing

```python
from helper.grobid_utils import process_pdfs_in_directory, grobid_process_pdf

# Process a directory of PDFs
process_pdfs_in_directory("data/pdfs", "data/xml_output")
```

Uses [GROBID](https://github.com/kermitt2/grobid) to convert academic PDF documents into structured TEI XML format.

---

### TEI XML Processing

```python
from helper.tei_utils import process_tei_xml_files, process_body_xml_to_plain_text

# Parse and clean TEI XML files into plain text
process_tei_xml_files()
process_body_xml_to_plain_text()
```

- `process_tei_xml_files()`: Extracts relevant metadata and structural content.
- `process_body_xml_to_plain_text()`: Converts the body text of XML documents into clean, retrievable plain text format for downstream RAG or NLP pipelines.

---

## ğŸš€ Use Case

This toolchain is especially useful in domain-specific RAG systems where clean, structured data from academic or technical PDFs is needed for building vector databases, knowledge bases, or context-aware LLM applications.

---

## âœ… Requirements

Install the required Python dependencies:

```bash
pip install -r requirements.txt
```

### `requirements.txt`

```
lxml
beautifulsoup4
tqdm
requests
```

Youâ€™ll also need a running [GROBID](https://github.com/kermitt2/grobid) server (Java-based). Start it using:

```bash
./gradlew run
```

Or with Docker:

```bash
docker run -t --rm -p 8070:8070 lfoppiano/grobid
```

---

## ğŸ’¬ Flask-Based RAG Chat Server

After running the data pre-processing pipeline (using GROBID and TEI utilities), you can launch an interactive RAG (Retrieval-Augmented Generation) system using the included Flask app.

### ğŸ§  Features

- Uses [LangChain](https://github.com/langchain-ai/langchain) with Chroma for vector search  
- Embedding model: `BAAI/bge-m3` via `sentence-transformers`  
- Automatically loads and splits `.pdf` and `.txt` files  
- Domain-specific prompt templates (e.g., dementia care)  
- LM Studio support with both streaming and non-streaming chat  
- Context citations and basic emergency tagging  

### ğŸ—‚ï¸ Structure

```
rag_app/
â”œâ”€â”€ app.py                      # Main Flask application
â”œâ”€â”€ services/                   # Modular service files
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ embedding_utils.py
â”‚   â”œâ”€â”€ file_utils.py
â”‚   â”œâ”€â”€ vectorstore_utils.py
â”‚   â””â”€â”€ llm_utils.py
â””â”€â”€ Plain_Text/                 # Input files for processing (PDFs or TXT)
```

### ğŸš€ Getting Started

```bash
cd rag_app
pip install -r requirements.txt
python app.py
```

### ğŸ›  Additional Requirements

These packages are needed for the RAG app portion:

```
Flask
torch
langchain
langchain-community
chromadb
sentence-transformers
PyPDF2
requests
```

Install them with:

```bash
pip install Flask torch langchain langchain-community chromadb sentence-transformers PyPDF2 requests
```

### â–¶ï¸ How to Start the RAG App

1. Navigate to the app folder:
   ```bash
   cd rag_app
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Add your `.txt` or `.pdf` files to the `Plain_Text/` folder:
   ```
   rag_app/
   â””â”€â”€ Plain_Text/
       â”œâ”€â”€ example1.txt
       â””â”€â”€ example2.pdf
   ```

4. Start LM Studio (or another local LLM endpoint) at:
   ```
   http://localhost:1234/v1/chat/completions
   ```

5. Launch the Flask app:
   ```bash
   python app.py
   ```

6. Send requests to the API:
   ```json
      curl --location 'http://localhost:5000/v1/chat/completions' \
      --header 'x-api-key: your_secure_api_key' \
      --header 'Content-Type: application/json' \
      --data '{"messages": [{"role": "user", "content": "What are tiny pointers?"}], "stream": false, "include_citations": true}'
   ```

---

## ğŸ§© Question-Answer Generation Pipeline Helper Module

This module provides a fully automated pipeline for generating scenario-based **questions and answers** using a **local LLM (via LM Studio)** and a lightweight RAG setup. The output is structured and saved as a Pandas DataFrame for further use or export.

---

### ğŸ“¦ Features

- âœ… Generate **casual, practical questions** from context
- âœ… Generate **concise answers** using a RAG model/client
- âœ… Process `.json` files in bulk
- âœ… Handles malformed JSON responses with a built-in fixer
- âœ… Optionally includes annotation fields for manual review

---

### ğŸ“ Input Format

Each input `.json` file must contain a list of dictionaries with at least a `Chunk` field. Optionally include a `Title`.

```json
[
  {
    "Title": "Understanding ML",
    "Chunk": "Machine learning is a method of data analysis that automates analytical model building."
  }
]
```

---

### ğŸš€ How to Use

#### 1. **Import the module**
```python
from helpers.qag_util import process_all
```

#### 2. **Run the pipeline**
```python
df = process_all("path/to/your/json/files")
df.to_csv("generated_qa.csv", index=False)
```

---

### ğŸ“ Output Columns

| Column      | Description                                                   |
|-------------|---------------------------------------------------------------|
| `question`  | Generated scenario-based question                             |
| `answer`    | Answer generated using a second client (e.g. local RAG app)   |
| `head`      | Title of the context chunk (if provided)                      |
| `text`      | Original input text chunk                                     |

---

### âš™ï¸ Requirements

- [LM Studio](https://lmstudio.ai/) running locally with a chat-compatible LLM
- Previously Mentioned RAG app
- Python packages:
  - `pandas`
  - `nltk`
  - `langchain`
  - `scipy`, `sklearn`
  - `tqdm`
  - `pydantic`
- Download required NLTK data:
```python
import nltk
nltk.download('punkt')
```

---

## ğŸ§ Future Extensions

- Add support for metadata indexing (authors, year, abstract).
- Integrate chunking and embedding pipeline.
- Add CLI and GUI wrappers for easier batch processing.

---

## ğŸ¤ Contributing

Feel free to open issues or submit PRs if youâ€™d like to contribute!

---

## ğŸ“„ License

MIT License

