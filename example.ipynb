{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.grobid_utils import process_pdfs_in_directory, grobid_process_pdf\n",
    "\n",
    "# Process a directory of PDFs\n",
    "process_pdfs_in_directory(\"data/pdfs\", \"data/xml_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.tei_utils import process_tei_xml_files, process_body_xml_to_plain_text\n",
    "\n",
    "process_tei_xml_files()\n",
    "process_body_xml_to_plain_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generateQA(data/chuncks.json, apiendpoint=localhost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.qag_utils import process_all\n",
    "import pandas as pd\n",
    "\n",
    "# Path to folder containing your JSON input files\n",
    "data_directory = \"data/json/\"\n",
    "\n",
    "# Run the full pipeline\n",
    "df = process_all(data_directory)\n",
    "\n",
    "# Preview results\n",
    "print(df.head())\n",
    "df.describe()\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"generated_qa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune_utils import (\n",
    "    login_to_huggingface, load_model_and_tokenizer, add_lora_adapters,\n",
    "    prepare_dataset, get_training_args, run_training, save_and_push_model\n",
    ")\n",
    "\n",
    "# Setup\n",
    "login_to_huggingface()\n",
    "\n",
    "model_name = \"your-model-name\"  # e.g., \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "save_path = \"./your/local/save/dir\"\n",
    "csv_path = \"./your-dataset.csv\"\n",
    "output_dir = \"./your/output/dir\"\n",
    "hf_repo = \"your-org/your-model-repo\"\n",
    "\n",
    "# Load model/tokenizer, apply LoRA, prepare data\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, save_path, use_quantization=True)\n",
    "model = add_lora_adapters(model)\n",
    "train_dataset, eval_dataset = prepare_dataset(csv_path, tokenizer)\n",
    "training_args = get_training_args(output_dir)\n",
    "\n",
    "# Train\n",
    "trainer = run_training(model, tokenizer, train_dataset, eval_dataset, training_args)\n",
    "\n",
    "# Save merged model & push to hub\n",
    "save_and_push_model(trainer.model, tokenizer, merged_dir=output_dir + \"-merged\", hf_repo=hf_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune_utils import convert_to_gguf\n",
    "\n",
    "# After training and merging:\n",
    "merged_model_path = \"./models/YourProject/Llama-1B-Merged\"\n",
    "gguf_output_path = \"./models/YourProject/gguf-out\"\n",
    "\n",
    "# requires llama-cpp\n",
    "convert_to_gguf(\n",
    "    input_dir=merged_model_path,\n",
    "    output_dir=gguf_output_path,\n",
    "    dtype=\"q8_0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from helper_functions import (\n",
    "    load_and_prepare_data,\n",
    "    generate_model_responses,\n",
    "    evaluate_scores,\n",
    "    visualize_scores,\n",
    "    calculate_improvements,\n",
    "    generate_wordclouds,\n",
    ")\n",
    "from IPython.display import display\n",
    "\n",
    "# Paths and setup\n",
    "test_data_path = \"questions_answers_comparisons_updated.csv\"\n",
    "output_file = \"test_data_sample_with_generated_answers.csv\"\n",
    "\n",
    "# Initialize OpenAI clients\n",
    "direct_client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "rag_client = OpenAI(base_url=\"http://127.0.0.1:5000\", api_key=\"lm-studio\")\n",
    "\n",
    "# Model configurations\n",
    "models = [\n",
    "    (\"Direct: meta-llama\", \"meta-llama-3.1-8b-instruct\", direct_client),\n",
    "    (\"Direct: med-llama\", \"med-llama-3.1-8b-instruct-guff\", direct_client),\n",
    "    (\"RAG: meta-llama\", \"meta-llama-3.1-8b-instruct@q8_0\", rag_client),\n",
    "    (\"RAG: med-llama\", \"med-llama-3.1-8b-instruct-guff@q8_0\", rag_client),\n",
    "]\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "test_data_sample = load_and_prepare_data(test_data_path, output_file)\n",
    "\n",
    "# Step 2: Generate responses\n",
    "test_data_sample = generate_model_responses(test_data_sample, models, output_file)\n",
    "\n",
    "# Step 3: Evaluate scores\n",
    "results = evaluate_scores(test_data_sample, models)\n",
    "\n",
    "# Step 4: Visualize scores\n",
    "visualize_scores(results)\n",
    "\n",
    "# Step 5: Calculate and display improvements\n",
    "improvement_table = calculate_improvements(results, baseline_index=0)\n",
    "display(improvement_table)\n",
    "\n",
    "# Step 6: Word cloud visualization\n",
    "generate_wordclouds(test_data_sample, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
